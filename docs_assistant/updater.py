import os
import json
import time
import hashlib
import requests
import logging
from datetime import datetime
import yaml
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('docs-updater')

# ç¯å¢ƒå˜é‡é…ç½®
UPDATE_INTERVAL = int(os.environ.get('UPDATE_INTERVAL', 1800))  # é»˜è®¤30åˆ†é’Ÿ
GITHUB_REPO = os.environ.get('GITHUB_REPO', 'Calcium-Ion/new-api')
CACHE_DIR = os.environ.get('CACHE_DIR', '/app/docs/.cache')
GITHUB_PROXY = os.environ.get('GITHUB_PROXY', 'https://api2.aimage.cc/proxy')
USE_PROXY = os.environ.get('USE_PROXY', 'true').lower() == 'true'
DOCS_DIR = os.environ.get('DOCS_DIR', '/app/docs')

# ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
os.makedirs(CACHE_DIR, exist_ok=True)

def get_cache_path(repo, data_type, count):
    """è·å–ç¼“å­˜æ–‡ä»¶çš„è·¯å¾„"""
    safe_repo_name = repo.replace("/", "_")
    return os.path.join(CACHE_DIR, f"{safe_repo_name}_{data_type}_{count}.json")

def get_data_hash(data):
    """è·å–æ•°æ®çš„å“ˆå¸Œå€¼"""
    if isinstance(data, list):
        hash_data = []
        for item in data[:10]:  # åªä½¿ç”¨å‰10ä¸ªé¡¹ç›®è®¡ç®—å“ˆå¸Œ
            if isinstance(item, dict):
                if 'tag_name' in item:
                    hash_data.append(f"{item.get('tag_name')}_{item.get('created_at')}")
                elif 'login' in item:
                    hash_data.append(f"{item.get('login')}_{item.get('contributions')}")
        return hashlib.md5(json.dumps(hash_data).encode()).hexdigest()
    return hashlib.md5(json.dumps(data).encode()).hexdigest()

def fetch_github_data(repo, data_type, count, use_proxy=True):
    """è·å–GitHubæ•°æ®"""
    logger.info(f"è·å–GitHubæ•°æ®: {repo}, {data_type}, count={count}")
    
    headers = {'User-Agent': 'Mozilla/5.0 DocUpdater/1.0'}
    
    try:
        # æ„å»ºAPIè·¯å¾„
        if data_type == "releases":
            api_path = f'repos/{repo}/releases?per_page={count}'
        elif data_type == "contributors":
            api_path = f'repos/{repo}/contributors?per_page={count}'
        else:
            return None, False
        
        # æ„å»ºAPI URL
        if use_proxy and USE_PROXY:
            original_api_url = f'https://api.github.com/{api_path}'
            api_url = f'{GITHUB_PROXY}?url={original_api_url}'
        else:
            api_url = f'https://api.github.com/{api_path}'
        
        # å‘é€è¯·æ±‚
        response = requests.get(api_url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # å¤„ç†åˆ†é¡µ (å¦‚æœéœ€è¦)
        if data_type == "contributors" and len(data) < count and len(data) > 0:
            all_data = data.copy()
            page = 2
            
            # æœ€å¤šè·å–5é¡µ
            while len(all_data) < count and page <= 5:
                # æ„å»ºä¸‹ä¸€é¡µURL
                next_api_url = f'{api_path}&page={page}'
                if use_proxy and USE_PROXY:
                    next_url = f'{GITHUB_PROXY}?url=https://api.github.com/{next_api_url}'
                else:
                    next_url = f'https://api.github.com/{next_api_url}'
                
                next_response = requests.get(next_url, headers=headers, timeout=30)
                next_response.raise_for_status()
                next_data = next_response.json()
                
                if not next_data:
                    break
                    
                all_data.extend(next_data)
                page += 1
                time.sleep(1)  # é¿å…è§¦å‘é™åˆ¶
            
            return all_data[:count], True
        
        return data, True
    
    except requests.exceptions.RequestException as e:
        logger.error(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
        
        # å¦‚æœä»£ç†å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®
        if use_proxy and USE_PROXY:
            logger.info("ä»£ç†è¯·æ±‚å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®")
            return fetch_github_data(repo, data_type, count, False)
        
        return None, False
    except Exception as e:
        logger.error(f"è·å–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None, False

def update_cache(repo, data_type, count):
    """æ›´æ–°ç¼“å­˜æ–‡ä»¶"""
    cache_file = get_cache_path(repo, data_type, count)
    cache_changed = False
    
    try:
        # è·å–æ–°æ•°æ®
        new_data, success = fetch_github_data(repo, data_type, count)
        if not success or not new_data:
            logger.error(f"æ— æ³•è·å– {repo} çš„ {data_type} æ•°æ®")
            return False
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_file):
            # åŠ è½½ç°æœ‰æ•°æ®
            with open(cache_file, 'r', encoding='utf-8') as f:
                old_data = json.load(f)
            
            # æ¯”è¾ƒæ•°æ®æ˜¯å¦æœ‰å˜åŒ–
            if get_data_hash(old_data) == get_data_hash(new_data):
                logger.info(f"{repo} çš„ {data_type} æ•°æ®æ²¡æœ‰å˜åŒ–ï¼Œè·³è¿‡æ›´æ–°")
                return False
            else:
                logger.info(f"{repo} çš„ {data_type} æ•°æ®æœ‰å˜åŒ–ï¼Œæ›´æ–°ç¼“å­˜")
                cache_changed = True
        else:
            logger.info(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»º {repo} çš„ {data_type} ç¼“å­˜")
            cache_changed = True
        
        # æ›´æ–°ç¼“å­˜æ–‡ä»¶
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(new_data, f)
        
        logger.info(f"å·²æ›´æ–° {repo} çš„ {data_type} ç¼“å­˜")
        return cache_changed
    
    except Exception as e:
        logger.error(f"æ›´æ–°ç¼“å­˜å¤±è´¥: {str(e)}")
        return False

def update_mkdocs_timestamp():
    """æ›´æ–°MkDocsé…ç½®æ–‡ä»¶æ—¶é—´æˆ³ï¼Œè§¦å‘é‡å»º"""
    try:
        # æŸ¥æ‰¾MkDocsé…ç½®æ–‡ä»¶
        config_file = os.path.join(DOCS_DIR, 'mkdocs.yml')
        if os.path.exists(config_file):
            # æ›´æ–°æ—¶é—´æˆ³
            os.utime(config_file, None)
            logger.info(f"å·²æ›´æ–°é…ç½®æ–‡ä»¶ {config_file} çš„æ—¶é—´æˆ³")
            return True
        else:
            logger.error(f"æœªæ‰¾åˆ°MkDocsé…ç½®æ–‡ä»¶: {config_file}")
            return False
    except Exception as e:
        logger.error(f"æ›´æ–°æ—¶é—´æˆ³å¤±è´¥: {str(e)}")
        return False

def format_contributors_markdown(contributors_data):
    """å°†è´¡çŒ®è€…æ•°æ®æ ¼å¼åŒ–ä¸ºMarkdownå†…å®¹"""
    if not contributors_data or len(contributors_data) == 0:
        return "æš‚æ— è´¡çŒ®è€…æ•°æ®ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    # ç”Ÿæˆè´¡çŒ®è€…å¡ç‰‡çš„HTML
    cards_html = '<div class="contributor-cards">\n'
    
    for idx, contributor in enumerate(contributors_data):
        login = contributor.get('login', 'æœªçŸ¥ç”¨æˆ·')
        avatar_url = contributor.get('avatar_url', '')
        html_url = contributor.get('html_url', '#')
        contributions = contributor.get('contributions', 0)
        
        # æ ¹æ®æ’åæ·»åŠ ä¸åŒçš„è¾¹æ¡†æ ·å¼
        border_class = ""
        if idx == 0:
            border_class = "gold-border"  # é‡‘ç‰Œ
        elif idx == 1:
            border_class = "silver-border"  # é“¶ç‰Œ
        elif idx == 2:
            border_class = "bronze-border"  # é“œç‰Œ
        
        cards_html += f'''
<div class="contributor-card {border_class}">
  <div class="contributor-avatar">
    <a href="{html_url}" target="_blank">
      <img src="{avatar_url}" alt="{login}">
    </a>
  </div>
  <div class="contributor-info">
    <div class="contributor-name"><a href="{html_url}" target="_blank">{login}</a></div>
    <div class="contributor-contributions">è´¡çŒ®: {contributions}</div>
  </div>
</div>
'''
    
    cards_html += '</div>\n'
    
    # æ·»åŠ CSSæ ·å¼
    css_style = '''
<style>
.contributor-cards {
  display: flex;
  flex-wrap: wrap;
  gap: 15px;
  justify-content: center;
  margin: 20px 0;
}

.contributor-card {
  width: 130px;
  background-color: #f5f5f5;
  border-radius: 8px;
  padding: 10px;
  box-shadow: 0 2px 5px rgba(0,0,0,0.1);
  display: flex;
  flex-direction: column;
  align-items: center;
  transition: transform 0.2s;
}

.contributor-card:hover {
  transform: translateY(-5px);
  box-shadow: 0 5px 10px rgba(0,0,0,0.15);
}

.gold-border {
  border: 2px solid gold;
}

.silver-border {
  border: 2px solid silver;
}

.bronze-border {
  border: 2px solid #cd7f32;
}

.contributor-avatar {
  margin-bottom: 10px;
}

.contributor-avatar img {
  width: 60px;
  height: 60px;
  border-radius: 50%;
  object-fit: cover;
}

.contributor-info {
  text-align: center;
}

.contributor-name {
  font-weight: bold;
  margin-bottom: 5px;
}

.contributor-contributions {
  font-size: 0.9em;
  color: #555;
}
</style>
'''
    
    # ç”Ÿæˆæœ€ç»ˆçš„Markdownå†…å®¹
    markdown_content = f'''## ğŸ‘¨â€ğŸ’» å¼€å‘è´¡çŒ®è€…

ä»¥ä¸‹æ˜¯æ‰€æœ‰ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…åˆ—è¡¨ã€‚åœ¨æ­¤æ„Ÿè°¢ä»–ä»¬çš„è¾›å‹¤å·¥ä½œå’Œåˆ›æ„ï¼

!!! info "è´¡çŒ®è€…ä¿¡æ¯"
    ä»¥ä¸‹è´¡çŒ®è€…æ•°æ®ä» [GitHub Contributors é¡µé¢](https://github.com/{GITHUB_REPO}/graphs/contributors) è‡ªåŠ¨è·å–å‰{len(contributors_data)}åã€‚è´¡çŒ®åº¦å‰ä¸‰ååˆ†åˆ«ä»¥é‡‘ã€é“¶ã€é“œç‰Œè¾¹æ¡†æ ‡è¯†ã€‚å¦‚æœæ‚¨ä¹Ÿæƒ³ä¸ºé¡¹ç›®åšå‡ºè´¡çŒ®ï¼Œæ¬¢è¿æäº¤ Pull Requestã€‚

{css_style}
{cards_html}

> æœ€åæ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
'''
    
    return markdown_content

def format_releases_markdown(releases_data):
    """å°†å‘å¸ƒæ•°æ®æ ¼å¼åŒ–ä¸ºMarkdownå†…å®¹"""
    if not releases_data or len(releases_data) == 0:
        return "æš‚æ— ç‰ˆæœ¬æ•°æ®ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    markdown_content = "# ğŸ“ æ›´æ–°æ—¥å¿—\n\n"
    markdown_content += "!!! warning \"æ›´å¤šç‰ˆæœ¬\"\n"
    markdown_content += f"    å¦‚éœ€æŸ¥çœ‹å…¨éƒ¨å†å²ç‰ˆæœ¬ï¼Œè¯·è®¿é—® [GitHub Releases é¡µé¢](https://github.com/{GITHUB_REPO}/releases)ï¼Œæœ¬é¡µé¢ä»è¯¥é¡µé¢å®šæ—¶è·å–æœ€æ–°æ›´æ–°ä¿¡æ¯ã€‚\n\n"
    
    for release in releases_data:
        tag_name = release.get('tag_name', 'æœªçŸ¥ç‰ˆæœ¬')
        name = release.get('name') or tag_name
        published_at = release.get('published_at', '')
        body = release.get('body', 'æ— å‘å¸ƒè¯´æ˜')
        
        if published_at:
            try:
                # è½¬æ¢ISOæ ¼å¼çš„æ—¶é—´ä¸ºæ›´å‹å¥½çš„æ ¼å¼
                pub_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                formatted_date = pub_date.strftime('%Y-%m-%d %H:%M:%S')
            except Exception:
                formatted_date = published_at
        else:
            formatted_date = 'æœªçŸ¥æ—¶é—´'
        
        # å¤„ç†Markdownæ ¼å¼
        body = body.replace('### ', '#### ').replace('## ', '### ')
        
        markdown_content += f"## {name}\n\n"
        markdown_content += f"å‘å¸ƒæ—¥æœŸ: {formatted_date}\n\n"
        markdown_content += f"{body}\n\n"
        markdown_content += "---\n\n"
    
    markdown_content += f"> æœ€åæ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    
    return markdown_content

def update_markdown_file(file_path, content, tag_pattern=None):
    """æ›´æ–°Markdownæ–‡ä»¶å†…å®¹"""
    try:
        original_content = ""
        file_changed = True
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
                
            # å¦‚æœæä¾›äº†æ ‡ç­¾æ¨¡å¼ï¼Œåˆ™åªæ›¿æ¢æ ‡ç­¾éƒ¨åˆ†
            if tag_pattern:
                match = re.search(tag_pattern, original_content, re.DOTALL)
                if match:
                    # åªæ›¿æ¢æ ‡ç­¾ä¹‹é—´çš„å†…å®¹
                    new_content = original_content[:match.start()] + content + original_content[match.end():]
                    
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦å®é™…æ›´æ”¹
                    if new_content == original_content:
                        logger.info(f"æ–‡ä»¶ {file_path} å†…å®¹æœªæ›´æ”¹ï¼Œè·³è¿‡å†™å…¥")
                        file_changed = False
                    else:
                        # å†™å…¥æ–°å†…å®¹
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(new_content)
                        logger.info(f"å·²æ›´æ–°æ–‡ä»¶ {file_path} çš„æ ‡ç­¾å†…å®¹")
                else:
                    # æ ‡ç­¾æœªæ‰¾åˆ°ï¼Œç›´æ¥è¿½åŠ å†…å®¹
                    logger.warning(f"æ–‡ä»¶ {file_path} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ ‡ç­¾ï¼Œå°†è¿½åŠ å†…å®¹")
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(original_content + "\n\n" + content)
            else:
                # å¦‚æœæ²¡æœ‰æ ‡ç­¾æ¨¡å¼ï¼Œç›´æ¥è¦†ç›–æ•´ä¸ªæ–‡ä»¶
                if original_content != content:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    logger.info(f"å·²è¦†ç›–æ›´æ–°æ–‡ä»¶ {file_path}")
                else:
                    logger.info(f"æ–‡ä»¶ {file_path} å†…å®¹æœªæ›´æ”¹ï¼Œè·³è¿‡å†™å…¥")
                    file_changed = False
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"å·²åˆ›å»ºæ–‡ä»¶ {file_path}")
        
        return file_changed
    except Exception as e:
        logger.error(f"æ›´æ–°Markdownæ–‡ä»¶å¤±è´¥: {str(e)}")
        return False

def update_documents_with_github_data():
    """ä½¿ç”¨ç¼“å­˜çš„GitHubæ•°æ®æ›´æ–°æ–‡æ¡£"""
    changes_detected = False
    
    # æ›´æ–°å‘å¸ƒæ—¥å¿—
    releases_cache = get_cache_path(GITHUB_REPO, "releases", 30)
    if os.path.exists(releases_cache):
        try:
            with open(releases_cache, 'r', encoding='utf-8') as f:
                releases_data = json.load(f)
            
            # æ ¼å¼åŒ–ä¸ºMarkdown
            releases_markdown = format_releases_markdown(releases_data)
            
            # æ›´æ–°åˆ°æ–‡ä»¶
            changelog_file = os.path.join(DOCS_DIR, 'docs/wiki/changelog.md')
            if update_markdown_file(changelog_file, releases_markdown):
                changes_detected = True
                logger.info("å·²æ›´æ–°æ›´æ–°æ—¥å¿—")
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ›´æ–°æ—¥å¿—å¤±è´¥: {str(e)}")
    
    # æ›´æ–°è´¡çŒ®è€…åˆ—è¡¨
    contributors_cache = get_cache_path(GITHUB_REPO, "contributors", 50)
    if os.path.exists(contributors_cache):
        try:
            with open(contributors_cache, 'r', encoding='utf-8') as f:
                contributors_data = json.load(f)
            
            # æ ¼å¼åŒ–ä¸ºMarkdown
            contributors_markdown = format_contributors_markdown(contributors_data)
            
            # æ›´æ–°åˆ°æ–‡ä»¶
            thanks_file = os.path.join(DOCS_DIR, 'docs/wiki/special-thanks.md')
            if os.path.exists(thanks_file):
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(thanks_file, 'r', encoding='utf-8') as f:
                    thanks_content = f.read()
                
                # æ‰¾åˆ°éœ€è¦æ›¿æ¢çš„éƒ¨åˆ†
                sections = thanks_content.split("## ğŸ‘¨â€ğŸ’» å¼€å‘è´¡çŒ®è€…")
                if len(sections) > 1:
                    # æå–ç¬¬ä¸€éƒ¨åˆ†
                    first_part = sections[0].strip()
                    
                    # æ‹¼æ¥æ–°å†…å®¹
                    new_content = f"{first_part}\n\n{contributors_markdown}"
                    
                    # æ›´æ–°æ–‡ä»¶
                    if update_markdown_file(thanks_file, new_content):
                        changes_detected = True
                        logger.info("å·²æ›´æ–°è´¡çŒ®è€…åˆ—è¡¨")
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°åˆ†éš”æ ‡è®°ï¼Œç›´æ¥æ·»åŠ å†…å®¹
                    update_markdown_file(thanks_file, contributors_markdown)
                    changes_detected = True
                    logger.info("å·²æ·»åŠ è´¡çŒ®è€…åˆ—è¡¨")
            else:
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºåŒ…å«å®Œæ•´å†…å®¹çš„æ–‡ä»¶
                full_content = f"# New API çš„å¼€å‘ç¦»ä¸å¼€ç¤¾åŒºçš„æ”¯æŒå’Œè´¡çŒ®ã€‚åœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®æä¾›å¸®åŠ©çš„ä¸ªäººå’Œç»„ç»‡ã€‚\n\n{contributors_markdown}"
                update_markdown_file(thanks_file, full_content)
                changes_detected = True
                logger.info("å·²åˆ›å»ºè´¡çŒ®è€…åˆ—è¡¨æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"æ›´æ–°è´¡çŒ®è€…åˆ—è¡¨å¤±è´¥: {str(e)}")
    
    return changes_detected

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨æ–‡æ¡£æ›´æ–°æœåŠ¡")
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    os.makedirs(CACHE_DIR, exist_ok=True)
    logger.info(f"ç¼“å­˜ç›®å½•: {CACHE_DIR}")
    
    # æ£€æŸ¥MkDocsé…ç½®æ–‡ä»¶
    config_file = os.path.join(DOCS_DIR, 'mkdocs.yml')
    if os.path.exists(config_file):
        logger.info(f"æ‰¾åˆ°MkDocsé…ç½®æ–‡ä»¶: {config_file}")
    else:
        logger.warning(f"æœªæ‰¾åˆ°MkDocsé…ç½®æ–‡ä»¶: {config_file}")
    
    # è®¾ç½®åˆå§‹æ›´æ–°æ—¶é—´
    last_check = 0
    
    # ä¸»å¾ªç¯
    while True:
        try:
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if current_time - last_check >= UPDATE_INTERVAL:
                logger.info("å¼€å§‹æ£€æŸ¥æ›´æ–°")
                last_check = current_time
                
                # éœ€è¦æ›´æ–°çš„æ•°æ®
                update_tasks = [
                    (GITHUB_REPO, "releases", 30),
                    (GITHUB_REPO, "contributors", 50)
                ]
                
                cache_changes_detected = False
                
                # æ‰§è¡Œæ›´æ–°
                for repo, data_type, count in update_tasks:
                    cache_changed = update_cache(repo, data_type, count)
                    if cache_changed:
                        cache_changes_detected = True
                    time.sleep(5)  # é¿å…è¿ç»­è¯·æ±‚
                
                # ä½¿ç”¨GitHubæ•°æ®æ›´æ–°æ–‡æ¡£
                doc_changes_detected = update_documents_with_github_data()
                
                # å¦‚æœæœ‰å˜åŒ–ï¼Œè§¦å‘MkDocsé‡å»º
                if cache_changes_detected or doc_changes_detected:
                    logger.info("æ£€æµ‹åˆ°å˜åŒ–ï¼Œè§¦å‘MkDocsé‡å»º")
                    update_mkdocs_timestamp()
                else:
                    logger.info("æ²¡æœ‰æ£€æµ‹åˆ°å˜åŒ–ï¼Œè·³è¿‡è§¦å‘é‡å»º")
            
            # ä¼‘çœ ä¸€æ®µæ—¶é—´
            sleep_time = 60  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦æ›´æ–°
            logger.debug(f"ä¼‘çœ  {sleep_time} ç§’")
            time.sleep(sleep_time)
            
        except Exception as e:
            logger.error(f"æ›´æ–°å¾ªç¯å‡ºé”™: {str(e)}")
            time.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿå†é‡è¯•

if __name__ == "__main__":
    main()